{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import graph data into a pandas dataframe\n",
    "df_dblp = pd.read_csv(r\"D:\\Facultate\\Year 2\\Semester 1\\Social Network Analysis for CS\\main project\\data\\com-dblp.ungraph.txt\", \n",
    "                 sep='\\t') # Dont use hardcoded paths, what we did is create a zip of the data, but it is too big so you can just add it to drive or something and then unzip it in the project when you need to run the code.\n",
    "\n",
    "# rename columns\n",
    "df_dblp.rename(columns={'# FromNodeId': 'source', 'ToNodeId': 'target'}, inplace=True) # not all datasets have this format, so will need to change\n",
    "\n",
    "# create networkx graph\n",
    "g_dblp = nx.from_pandas_edgelist(df_dblp, \"source\", \"target\", create_using = nx.Graph)\n",
    "\n",
    "# I would highly recommend using a more object oriented approach for the code, because now you have to take naming into account and you've got two files and a main file with the same code.\n",
    "# Also don't forget to clean up unused packages etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_statistics(g):\n",
    "    n_nodes = g.number_of_nodes()\n",
    "    n_edges = len(list(g.edges())) # Isn't there also a number_of_edges() method?\n",
    "    density = nx.density(g)\n",
    "    avg_clustering = nx.average_clustering(g)\n",
    "\n",
    "    \n",
    "    stats = pd.DataFrame()\n",
    "    stats[\"name\"] = [\"n_nodes\", \"n_edges\",\"density\",\"avg_clustering\"]\n",
    "    stats[\"values\"] = [n_nodes, n_edges, density, avg_clustering]\n",
    "    \n",
    "    return stats\n",
    "    \n",
    "stats = get_graph_statistics(g_dblp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 1 - Iteratively removing covered nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy1(graph, alpha = 0.00005): # Maybe come up with a name for your strategies so they are easier to identify\n",
    "    \"\"\"\n",
    "    This function selects landmarks and computes the distances from each landmark to every reachable node in the graph.\n",
    "    \n",
    "    :param graph (nx.Graph): the networkx graph created from the edgelist\n",
    "    :param alpha (float): landmark scaling factor. This variable controls the number of landmarks selected at every iteration.\n",
    "                          The number of landmarks selected is computed as following: N = int(num_remaining_nodes * alpha)\n",
    "\n",
    "    :returns:\n",
    "      - distance_mapping (pandas.core.frame.DataFrame): a pandas DataFrame having as index column the list of all nodes in\n",
    "                                                        the graph. Each column other than the index column is denoted by a\n",
    "                                                        landmark and contains the distance from the landmark to every other node.\n",
    "                                                        If a node is not reachable by a landmark, the value in the cell will \n",
    "                                                        be NaN.\n",
    "                                \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the nodelist\n",
    "    nodelist = np.array(list(graph.nodes())) # Currently not used\n",
    "    \n",
    "    # initialize set of landmarks\n",
    "    landmark_nodes = []\n",
    "\n",
    "    # rank the nodes based on degree centrality\n",
    "    ranking = dict(sorted(nx.degree_centrality(graph).items(), key=operator.itemgetter(1),reverse=True))\n",
    "    \n",
    "    # create a list of nodes ordered by their degree centrality\n",
    "    ranking_list = list(ranking)\n",
    "    \n",
    "    current_n_landmarks = 0\n",
    "    \n",
    "    # this variable will contain the distances between landmarks and all other nodes\n",
    "    # if a node is not reachable, the distance will be set to NaN\n",
    "    distance_mapping = pd.DataFrame()\n",
    "    distance_mapping[\"vertices\"] = list(graph.nodes())\n",
    "    distance_mapping = distance_mapping.set_index(\"vertices\")\n",
    "    \n",
    "    while len(ranking_list) > 0:\n",
    "        u_list = ranking_list[:int(len(ranking_list)*alpha)]\n",
    "        \n",
    "        for u in tqdm(u_list):\n",
    "            # add u to the list of landmark nodes\n",
    "            landmark_nodes.append(u)\n",
    "            \n",
    "            # get the distance from \"u\" to all other nodes\n",
    "            shortest_path_lengths = nx.single_source_shortest_path_length(graph, u)\n",
    "            sp_array = np.array(list(shortest_path_lengths.items()))\n",
    "            \n",
    "            # get an array of reached nodes\n",
    "            reached_nodes = list(shortest_path_lengths.keys())\n",
    "            recorded_distances  = list(shortest_path_lengths.values())\n",
    "            \n",
    "            df_u = pd.DataFrame()\n",
    "            df_u[\"vertices\"] = reached_nodes\n",
    "            df_u[str(u)] = recorded_distances\n",
    "            \n",
    "            distance_mapping = distance_mapping.join(df_u.set_index(\"vertices\"))\n",
    "            \n",
    "            # compute the average distance\n",
    "            average_distance = sp_array[:, 1].mean()\n",
    "\n",
    "            # get the nodes within average distance\n",
    "            nodes_in_range = list(sp_array[np.where(sp_array[:,1] <= average_distance)[0], :][:,0])\n",
    "\n",
    "            # remove the nodes that are within average distance\n",
    "            updated_ranking_list = set(ranking_list) - set(nodes_in_range) # Maybe don't convert to the ranking list and just use the dict so you don't have to convert back\n",
    "            ranking_list = list(updated_ranking_list)\n",
    "                \n",
    "\n",
    "        new_n_landmarks = len(landmark_nodes)\n",
    "        \n",
    "        if new_n_landmarks == current_n_landmarks:\n",
    "            break\n",
    "        else:\n",
    "            current_n_landmarks = new_n_landmarks\n",
    "\n",
    "    return distance_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:32<00:00,  1.29s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "distance_mapping = strategy1(g_dblp, alpha = 0.00008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2 -  Community partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import louvain_communities, asyn_fluidc, girvan_newman\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy2(graph, k = 50, seed=123):\n",
    "    \"\"\"\n",
    "    This function implements strategy 2 using Fluid Community detection algorithm.\n",
    "    \n",
    "    :param graph (nx.Graph): the networkx graph created from the edgelist.\n",
    "    :param k (int): indicates the number of communities that need to be found.\n",
    "    :param seed (int): the random seed used in the community detection algorithm.\n",
    "    \n",
    "    :returns:\n",
    "      - distance_mapping (pandas.core.frame.DataFrame): a pandas DataFrame having as index column the list of all nodes in\n",
    "                                                        the graph. Each column other than the index column is denoted by a\n",
    "                                                        landmark and contains the distance from the landmark to every other node.\n",
    "                                                        If a node is not reachable by a landmark, the value in the cell will \n",
    "                                                        be infinity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # search for communities\n",
    "    \n",
    "    ##### Using Louvain Communities #####\n",
    "#     landmark_communities = louvain_communities(graph, resolution=0.6, seed=123) \n",
    "\n",
    "    ##### Using fluid communities #####\n",
    "    landmark_communities  = list(asyn_fluidc(graph, k = k, seed=seed))\n",
    "    \n",
    "    # obtain the size of each community\n",
    "    community_sizes = np.array([len(comm) for comm in landmark_communities])\n",
    "\n",
    "    num_landmarks = [math.ceil(100*l/sum(community_sizes)) for l in community_sizes]\n",
    "\n",
    "    # obtain the degree centrality of all nodes in the graph\n",
    "    degr_centrality_ranking = dict(sorted(nx.degree_centrality(graph).items(), key=operator.itemgetter(1),reverse=True))\n",
    "    \n",
    "    degr_centrality_of_communities = {}\n",
    "    \n",
    "    distance_mapping2 = pd.DataFrame()\n",
    "    distance_mapping2[\"vertices\"] = list(graph.nodes())\n",
    "    distance_mapping2 = distance_mapping2.set_index(\"vertices\") # Move this to the top of the for loop it is actually used for\n",
    "    \n",
    "    all_landmarks = []\n",
    "        \n",
    "    for i in range(len(landmark_communities)):\n",
    "        d = {key: degr_centrality_ranking.get(key) for key in landmark_communities[i]}\n",
    "\n",
    "        num_l = num_landmarks[i]\n",
    "\n",
    "        # rank the nodes in the community based on degree centrality\n",
    "        d = dict(sorted(d.items(), key=operator.itemgetter(1),reverse=True)[:num_l]) # Better name for d, even though it is the name in the paper\n",
    "        \n",
    "        all_landmarks += list(d.keys())\n",
    "\n",
    "        degr_centrality_of_communities[i] = d \n",
    "    \n",
    "    for k in degr_centrality_of_communities.keys():\n",
    "        landmarks = list(degr_centrality_of_communities[k].keys())\n",
    "\n",
    "        for u in landmarks:\n",
    "            shortest_path_lengths = nx.single_source_shortest_path_length(graph, u)\n",
    "\n",
    "            # get an array of reached nodes\n",
    "            reached_nodes = list(shortest_path_lengths.keys())\n",
    "            recorded_distances  = list(shortest_path_lengths.values())\n",
    "\n",
    "            df_u = pd.DataFrame()\n",
    "            df_u[\"vertices\"] = reached_nodes\n",
    "            df_u[str(u)] = recorded_distances # Doet is need to be a string?\n",
    "                               \n",
    "            df_u = df_u[df_u[\"vertices\"].isin(list(landmark_communities[k]) + all_landmarks)]\n",
    "\n",
    "            distance_mapping2 = distance_mapping2.join(df_u.set_index(\"vertices\"))\n",
    "    \n",
    "    # replace NaN values to infinity\n",
    "    distance_mapping2.fillna(np.inf, inplace = True) # In strategy 1 you use np.nan and here you use np.inf, why?\n",
    "    \n",
    "    return distance_mapping2, all_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "distance_mapping2, all_landmarks = strategy2(g_dblp, k = 50, seed=123)\n",
    "\n",
    "# get the dataframe containing the distances from landmark to landmark\n",
    "landmark_to_landmark = distance_mapping2.loc[all_landmarks, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
